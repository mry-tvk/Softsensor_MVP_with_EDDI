{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent Information Leakage: \n",
        "\n",
        "A typical ML downstream process involves input features predicting the output targets. Also, a test set is usually left out to do the evaluation. We make sure to prevent information leakage during evaluation in two aspects:\n",
        "<ul>\n",
        "  <li><strong>Testset to trainset</strong>: Only the trainset should be used to train the imputation model.</li>\n",
        "  <li><strong>Targets to predictors</strong>: To prevent data leakage from target features into predictive features, we have two options:</li>\n",
        "      <ol>\n",
        "        <li>Evaluate the imputation and downstream task as a package</li>\n",
        "        <li>Impute input and output columns of the dataset independently [as if they are separate datasets].</li>\n",
        "      </ol>\n",
        "      We use the second method in this notebook to focus our evaluate on the imputation peice only. Furthermore, if we need to seal outputs from each other we need to have 3 datasets in this case. Clearly, by having one column in a dataset, we cannot have a better result than just \"mean\"-imputation from EDDI. Therefore, at this point we only focus on <strong>input-imputation</strong>. On later notebooks, we will see that the temporal signals can help us with this issue.\n",
        "</ul>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install azure-storage-blob"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from helpers.dataprep_utils import compute_missing_ratio, get_variables_metadata\n",
        "from helpers.sas_utils import get_data_link, get_placeholder_link\n",
        "from helpers.service_utils import RestEDDI, train_eddi, batchinference_eddi\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import toml"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_sensors = pd.read_csv('./data_generated/sensor_wide.csv', index_col=0)\n",
        "df_sensors.head(1)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IN1</th>\n      <th>IN2</th>\n      <th>IN3</th>\n      <th>IN4</th>\n      <th>IN5</th>\n      <th>IN6</th>\n      <th>IN7</th>\n      <th>IN8</th>\n      <th>Out1</th>\n      <th>Out2</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.077744</td>\n      <td>0.795565</td>\n      <td>-0.665503</td>\n      <td>0.879321</td>\n      <td>0.134419</td>\n      <td>-1.133765</td>\n      <td>0.253945</td>\n      <td>0.109987</td>\n      <td>-0.122686</td>\n      <td>0.123661</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           IN1       IN2       IN3       IN4       IN5       IN6       IN7  \\\ntime                                                                         \n0     0.077744  0.795565 -0.665503  0.879321  0.134419 -1.133765  0.253945   \n\n           IN8      Out1      Out2  \ntime                                \n0     0.109987 -0.122686  0.123661  "
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load EDDI & Storage setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "config = toml.load(\"./config/config.toml\")\n",
        "\n",
        "container_sas_link = config['blob']['container_sas_link']\n",
        "subscription_key = config['eddi']['subscription_key']\n",
        "rest_eddi = RestEDDI(subscription_key, api_version=\"v2.3\")\n",
        "blob_storage_root = 'sensor_datasets' # name for the blob storage directory within the provided container to store the data"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep train and test datasets separate; assuming that on the downstream task we will take the first 0.7 part of the data as train and the rest as the test part; we will do the same here:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "splitpoint = df_sensors.index[int(df_sensors.shape[0] * 0.7)]\n",
        "df_train = df_sensors[df_sensors.index < splitpoint]\n",
        "df_test = df_sensors[df_sensors.index >= splitpoint]\n",
        "\n",
        "if 'train' not in os.listdir('./data_prepared/'):\n",
        "    os.mkdir('./data_prepared/train/')\n",
        "\n",
        "if 'test' not in os.listdir('./data_prepared/'):\n",
        "    os.mkdir('./data_prepared/test/')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impute input and output columns of the dataset independently [as if they are separate datasets]. Since we do not have an added value on the output, we skip that for now until future steps."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get variable metadata"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ideally we should have the metadata provided by the expert so we do not need to rely on current data distribution/min-max to infer the metadata; but we ignore this for now\n",
        "in_columns = [\"IN1\", \"IN2\", \"IN3\", \"IN4\", \"IN5\"]\n",
        "variables_metadata_in = get_variables_metadata(df_sensors, in_columns,'./config/variables_metadata_in.json')"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_in = df_train[in_columns].copy().fillna('')\n",
        "df_test_in = df_test[in_columns].copy().fillna('')\n",
        "\n",
        "df_train_in.to_csv('./data_prepared/train/sensor_wide_in.csv', index=False, header=False)\n",
        "df_test_in.to_csv('./data_prepared/test/sensor_wide_in.csv', index=False, header=False)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train EDDI imputation model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# uploads the train data to blob storage & returns the SAS link\n",
        "training_data_source = get_data_link('./data_prepared/train/sensor_wide_in.csv', container_sas_link, directory_name=blob_storage_root)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_input = {\n",
        "    \"training_data_source\": training_data_source,\n",
        "    \"model_hyperparams\":{\n",
        "        \"decoder_variances\": 1e-6\n",
        "    },\n",
        "    \"variables_metadata\": variables_metadata_in,\n",
        "    \"training_hyperparams\":{\n",
        "        \"epochs\": 500,\n",
        "        \"iterations\":40\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = train_eddi(train_input, rest_eddi)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<Response [200]>\n200\n'ab0bf8c5ab26453fa02704418405c516'\nOperation status: Completed\ntrain running time: 344.56353759765625 seconds\n{'created_time': '2022-04-07T00:51:26.737341+00:00',\n 'description': 'Model for efficient decision making with heterogenous data',\n 'id': 'ab0bf8c5ab26453fa02704418405c516'}\nhttps://ms-azua-api.azurewebsites.net/saas-api/models/ab0bf8c5ab26453fa02704418405c516?api-version=v2.3\n"
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch inferencing on train & test dataset and store the results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inference_input = {\n",
        "    \"hyperparameters\":\n",
        "    {\n",
        "        \"sample_count\": 50\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# batch-inference on train-dataset\n",
        "batch_inference_input[\"data_source\"] = training_data_source\n",
        "batch_inference_input[\"output\"] = get_placeholder_link('sensor_wide_in_impute.csv', container_sas_link, directory_name=blob_storage_root, delete_prev=True)\n",
        "batchinference_eddi(batch_inference_input, model_id, rest_eddi)\n",
        "\n",
        "# download and replace the train data with the missing data\n",
        "df_train_in_impute = pd.read_csv(batch_inference_input[\"data_source\"], names=in_columns)\n",
        "df_train_in_impute.to_csv('./data_prepared/train/sensor_wide_in.csv', header=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<Response [200]>\n200\n'aa6e29eba3c44d458e7d88f0acfd604d'\nOperation status: Completed\nbatchinference running time: 93.98148465156555 seconds\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# batch-inference on train-dataset\n",
        "batch_inference_input[\"data_source\"] = get_data_link('./data_prepared/test/sensor_wide_in.csv', container_sas_link, directory_name=blob_storage_root)\n",
        "batch_inference_input[\"output\"] = get_placeholder_link('sensor_wide_in_impute.csv', container_sas_link, directory_name=blob_storage_root, delete_prev=True)\n",
        "batchinference_eddi(batch_inference_input, model_id, rest_eddi)\n",
        "\n",
        "# download and replace the train data with the missing data\n",
        "df_test_in_impute = pd.read_csv(batch_inference_input[\"data_source\"], names=in_columns)\n",
        "df_test_in_impute.to_csv('./data_prepared/test/sensor_wide_in.csv', header=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<Response [200]>\n200\n'a679c64eea22447a8e4f147952020d5c'\nOperation status: Completed\nbatchinference running time: 105.16073083877563 seconds\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}